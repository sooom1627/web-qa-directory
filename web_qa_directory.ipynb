{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyMl9kT3KJelHdx6OsdeO3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sooom1627/web-qa-directory/blob/master/web_qa_directory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "A8MHglBs9Nmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "ellRjK0G9PwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import urllib.request\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import deque\n",
        "from html.parser import HTMLParser\n",
        "from urllib.parse import urlparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import openai\n",
        "from openai.embeddings_utils import distances_from_embeddings, cosine_similarity"
      ],
      "metadata": {
        "id": "laKgMHLH8kGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#APIキーを環境変数に設定\n",
        "os.environ['OPENAI_API_KEY'] = ''"
      ],
      "metadata": {
        "id": "Kz3JJRA2_jTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "O6gxjQCAAJr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#URLに適合する正規表現パターン\n",
        "HTTP_URL_PATTERN = r'^http[s]*://.+'"
      ],
      "metadata": {
        "id": "jjyJCtEV9rbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#クロールしたいwebサイトのドメインとURLを設定\n",
        "domain = \"\"\n",
        "full_url = \"\"\n",
        "\n",
        "# クロールするディレクトリを指定\n",
        "directory_path = \"\""
      ],
      "metadata": {
        "id": "gSknEvW39sBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HTMLを解析し、ハイパーリンクの一覧を取得するためのクラス定義\n",
        "class HyperlinkParser(HTMLParser):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.hyperlinks = []\n",
        "\n",
        "  ## ハイパーリンクを取得するために、HTMLParserのhandle_starttagメソッドをオーバーライド\n",
        "  def handle_starttag(self, tag, attrs):\n",
        "    attrs = dict(attrs)\n",
        "    #aタグかつhrefがある場合、辞書型のattrsからurlを抜き出し、リストに加える\n",
        "    if tag == \"a\" and \"href\" in attrs:\n",
        "      self.hyperlinks.append(attrs[\"href\"])\n",
        "\n",
        "#ハイパーリンクをURLから取得する関数を定義\n",
        "def get_hyperlinks(url):\n",
        "  #URLをオープンし、読み込む処理をtry\n",
        "  try:\n",
        "    # HTMLページを取得\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "      #もしContent-Typeがhtmlでなかった場合はからのリストを返却\n",
        "      if not response.info().get(\"Content-Type\", \"\").startswith(\"text/html\"):\n",
        "        return []\n",
        "      #htmlをutf-8形式にデコード\n",
        "      html = response.read().decode(\"utf-8\")\n",
        "  except requests.exceptions.HTTPError as e:\n",
        "    print(e)\n",
        "    return []\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(e)\n",
        "    return []\n",
        "\n",
        "  #HyperlinkParserクラスのオブジェクト生成\n",
        "  parser = HyperlinkParser()\n",
        "  #HTMLを解析し、ハイパーリンクのリストを生成\n",
        "  parser.feed(html)\n",
        "\n",
        "  return parser.hyperlinks"
      ],
      "metadata": {
        "id": "MSLZR-r9AP2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_domain_hyperlinks(local_domain, url, directory_path=\"\"):\n",
        "  #ドメインにあったURLを格納するためのからのリストを作成\n",
        "  clean_links = []\n",
        "  for link in set(get_hyperlinks(url)):\n",
        "    #ドメイン一致するリンクを格納する変数の初期化\n",
        "    clean_link = None\n",
        "\n",
        "    try:\n",
        "    #与えられたリンクがURLの正規表現にマッチするか判定\n",
        "      if re.search(HTTP_URL_PATTERN, link):\n",
        "        #リンクをパースし、URLの各部分をタプル形で格納\n",
        "        url_obj = urlparse(link)\n",
        "\n",
        "        #URLのネットワーク位置が取得したいドメインと一致しているか判別\n",
        "        if url_obj.netloc == local_domain:\n",
        "          #一致していた場合、ドメイン一致リンクとしてclean_link関数に格納\n",
        "          clean_link = link\n",
        "      else:\n",
        "        #相対パスで記述されているものを判定し、絶対パスへ変換するための前処理を行う\n",
        "        if link.startswith(\"/\"):\n",
        "          link = link[1:]\n",
        "\n",
        "        #リンクが#やmailto:などの場合、continueでループ処理をスキップ\n",
        "        elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
        "          continue\n",
        "        #相対パスから絶対パスを生成\n",
        "        clean_link = \"https://\" + local_domain + \"/\" + link\n",
        "    except:\n",
        "      print(\"miss\")\n",
        "      continue\n",
        "\n",
        "    #ここまでの処理でドメインと一致するURLリンクが得られていれば処理を実行\n",
        "    if clean_link is not None:\n",
        "      if clean_link.endswith(\"/\"):\n",
        "        clean_link = clean_link[:-1]\n",
        "\n",
        "      # directory_pathが空でない場合、リンクがそのディレクトリ内にあることを確認。そうでない場合は次のループへ\n",
        "      if directory_path and not clean_link.startswith(f\"https://{local_domain}/{directory_path}\"):\n",
        "        continue\n",
        "\n",
        "      clean_links.append(clean_link)\n",
        "\n",
        "  #最後にclean_linkリストを返却\n",
        "  return list(set(clean_links))"
      ],
      "metadata": {
        "id": "kE_zKOHQASSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ディレクトリ作成を関数化\n",
        "def create_directory_if_not_exists(directory):\n",
        "  if not os.path.exists(directory):\n",
        "    os.mkdir(directory)\n",
        "\n",
        "def crawl(url, directory_path=\"\"):\n",
        "  #URLをパースし、ドメインを取得\n",
        "  local_domain = urlparse(url).netloc\n",
        "\n",
        "  #もしディレクトリが指定されている場合は、ディレクトリパスを含めたurlへ上書き\n",
        "  if directory_path:\n",
        "    url = f\"https://{local_domain}/{directory_path}\"\n",
        "\n",
        "  #クロールするURLの一覧を格納するキューを作成\n",
        "  queue = deque([url])\n",
        "  #すでに処理済みのURLを格納するための変数を作成\n",
        "  seen = set([url])\n",
        "\n",
        "  #textディレクトリがない場合は作成\n",
        "  create_directory_if_not_exists(\"text/\")\n",
        "  #text/local_domainがない場合は作成\n",
        "  create_directory_if_not_exists(f\"text/{local_domain}/\")\n",
        "  #processedディレクトリを作成\n",
        "  create_directory_if_not_exists(\"processed\")\n",
        "\n",
        "  while queue:\n",
        "    url = queue.pop()\n",
        "\n",
        "    #text/local_domain配下にtxtファイルを作成\n",
        "    with open(f\"text/{local_domain}/{url[8:].replace('/', '_')}.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
        "      # BeautifulSoupを用いてHTMLを解析し、get_textメソッドでtextを抽出し\n",
        "      try:\n",
        "        response = requests.get(url, timeout=(3.0, 7.5))\n",
        "        time.sleep(0.2)\n",
        "\n",
        "        #レスポンスのステータスコードが200以外の場合ループをスキップ\n",
        "        if response.status_code != 200:\n",
        "          print(f\"Unable to fetch page {url}: Status code {response.status_code}\")\n",
        "          continue\n",
        "        #Content-Typeがtext/htmlでない場合処理をスキップ\n",
        "        content_type = response.headers.get('content-type')\n",
        "        if not content_type or not content_type.startswith(\"text/html\"):\n",
        "          continue\n",
        "        #レスポンスのHTMLを文字にデコード\n",
        "        html = response.content.decode(\"utf-8\")\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        #後のテキスト情報最適化のためにheaderタグを削除\n",
        "        header_tag = soup.find('header')\n",
        "        if header_tag:\n",
        "          header_tag.decompose()\n",
        "\n",
        "        #後のテキスト情報最適化のためにfooterタグを削除\n",
        "        footer_tag = soup.find('footer')\n",
        "        if footer_tag:\n",
        "          footer_tag.decompose()\n",
        "\n",
        "        #後のテキスト情報最適化のためにheadタグを削除\n",
        "        head_tag = soup.find('head')\n",
        "        if head_tag:\n",
        "          head_tag.decompose()\n",
        "\n",
        "        #htmlを文字情報として取得\n",
        "        text = soup.get_text()\n",
        "\n",
        "        #textの中に下記エラーあった場合は処理をスキップ\n",
        "        if (\"You need to enable JavaScript to run this app.\" in text):\n",
        "          print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
        "          continue\n",
        "        #抽出したテキストをファイルに書き込む\n",
        "        f.write(text)\n",
        "        print(f\"Able to fetch page {url}\")\n",
        "      except requests.exceptions.Timeout as e: # タイムアウトの例外処理\n",
        "        print(f\"Unable to fetch page {url}: Timeout\")\n",
        "        continue\n",
        "      except Exception as e:\n",
        "          print(f\"Fault to write text. Unable to fetch page {url}: {str(e)}\")\n",
        "          continue\n",
        "\n",
        "    #urlからHTMLに含まれるリンク一覧を取得し、queueに含め、queueに含めたリンクをseenに追加\n",
        "    for link in get_domain_hyperlinks(local_domain, url, directory_path):\n",
        "        if link not in seen:\n",
        "            queue.append(link)\n",
        "            seen.add(link)"
      ],
      "metadata": {
        "id": "JCEdkWOzAWlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawl(full_url, directory_path)"
      ],
      "metadata": {
        "id": "O_CoguRYBFsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_newlines(serie):\n",
        "  serie = serie.str.replace('\\n', ' ')\n",
        "  serie = serie.str.replace('\\\\n', ' ')\n",
        "  serie = serie.str.replace('  ', ' ')\n",
        "  serie = serie.str.replace('  ', ' ')\n",
        "  return serie"
      ],
      "metadata": {
        "id": "ISlpa7WXDO-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "\n",
        "#text/domai配下のファイルをひとつづつ取得\n",
        "for file in os.listdir(f\"text/{domain}/\"):\n",
        "  #テキスト情報に処理を加えながら、textsリストに追加\n",
        "  with open(f\"text/{domain}/{file}\", \"r\", encoding=\"UTF-8\") as f:\n",
        "    text = f.read()\n",
        "    texts.append((file[len(domain)+2:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))"
      ],
      "metadata": {
        "id": "Q43wEBTZDQih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#textsをdfに変換し、scraped.csvとしてcsvリスト化\n",
        "df = pd.DataFrame(texts, columns = [\"fname\", \"text\"])\n",
        "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
        "df.to_csv('processed/scraped.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2AEvvnCBDTs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ada-002モデルに適合するcl100k_baseトークナイザーをロード\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
      ],
      "metadata": {
        "id": "uX0pLLFJDpe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#processedディレクトリからcsvを読み込み、dfのカラムを変更する。\n",
        "df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
        "df.columns = ['title', 'text']\n",
        "\n",
        "#dfテキストをトークン化し、数を取得。その値を新しいn_tokensカラムに格納\n",
        "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))"
      ],
      "metadata": {
        "id": "LZ2I_T3SDs46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#トークンの数をヒストグラムで描画\n",
        "df.n_tokens.hist()"
      ],
      "metadata": {
        "id": "3yIHo7lSDyHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#トークンの最大数を500に設定\n",
        "max_tokens = 500\n",
        "\n",
        "def split_into_many(text, max_tokens = max_tokens):\n",
        "    #テキストをセンテンスに分割。日本語のサイトの場合は句点などで区切る必要がある。\n",
        "    sentences = text.split(\". \")\n",
        "    #リスト内包表記を用いてセンテンスごとのトークン数を取得\n",
        "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
        "\n",
        "    #分割されたtextを格納するchunksを作成\n",
        "    chunks = []\n",
        "    #作成中のチャンクに含まれるトークン数\n",
        "    tokens_so_far = 0\n",
        "    #現在のチャンクに含まれる文のリスト\n",
        "    chunk = []\n",
        "\n",
        "    #sentencesとn_tokensの2変数をzip関数を用いて同時にループ処理\n",
        "    for sentence, token in zip(sentences, n_tokens):\n",
        "      if tokens_so_far + token > max_tokens:\n",
        "        chunks.append(\". \".join(chunk) + \".\")\n",
        "        chunk = []\n",
        "        tokens_so_far = 0\n",
        "\n",
        "      if token > max_tokens:\n",
        "        continue\n",
        "\n",
        "      chunk.append(sentence)\n",
        "      tokens_so_far += token + 1\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "k-bLlJhWD3Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shortened = []\n",
        "\n",
        "for row in df.iterrows():\n",
        "  #row[0]はindex。row[1]のtupleのtextが存在するか確認\n",
        "  if row[1][\"text\"] is None:\n",
        "    continue\n",
        "  #row[1]のテクストがmax_tokensを超える場合、分割し、shortendに追加\n",
        "  if row[1][\"n_tokens\"] > max_tokens:\n",
        "    shortened += split_into_many(row[1]['text'])\n",
        "  else:\n",
        "    shortened.append( row[1]['text'] )\n",
        "\n",
        "#shortenedされた文字のリストから新たなdfを生成\n",
        "df = pd.DataFrame(shortened, columns = ['text'])\n",
        "#dfの各行ごとのtoken数を取得しn_token列に格納\n",
        "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
        "df.n_tokens.hist()\n",
        "\n",
        "#textの値を引数にopenAIのtext embedding関数に渡し、数値ベクトル化する。その値をembeddings列に追加する\n",
        "df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n",
        "#dfをembeddins.csvとして出力\n",
        "df.to_csv('processed/embeddings.csv')\n",
        "df.head()\n",
        "\n",
        "#embeddings.csvをdfとして読み込み\n",
        "df=pd.read_csv('processed/embeddings.csv', index_col=0)\n",
        "#文字列で保存されているembeddingsをevalでリスト化、さらにnp.arrayでnumpy配列にする\n",
        "df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "v0m0hWv_D6e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#データフレームから最も類似した文脈を見つけ、質問に対する文脈を作成する\n",
        "def create_context(question, df, max_len=1800, size=\"ada\"):\n",
        "\n",
        "    #与えられた質問を数値ベクトル化し、変数に格納\n",
        "    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
        "\n",
        "    #質問の埋め込みベクトルとデータフレーム内の各文の埋め込みベクトルとの間の距離を計算して、df データフレームに distances という列として追加\n",
        "    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n",
        "\n",
        "    #返却される質問を格納するリストを定義\n",
        "    returns = []\n",
        "    #文章のトークン数をカウントす変数を定義\n",
        "    cur_len = 0\n",
        "\n",
        "    #dfのdistanceを照準に並べ、ループ処理を実行\n",
        "    for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
        "\n",
        "        #トークンをカウントする変数に現在のrowのトークン数をかさん\n",
        "        cur_len += row['n_tokens'] + 4\n",
        "\n",
        "        #cur_lenがmax_lenを超えた場合、処理を終了\n",
        "        if cur_len > max_len:\n",
        "            break\n",
        "\n",
        "        #トークン数がmax_tokenを超えない場合、類似度が高い文章からreturnsに格納\n",
        "        returns.append(row[\"text\"])\n",
        "\n",
        "    #抽出された類似度の高いテキストを一つの文章として返却\n",
        "    return \"\\n\\n###\\n\\n\".join(returns)"
      ],
      "metadata": {
        "id": "DBxozPllEfBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#データフレームの中から質問に類似した文脈を取得し、回答を取得する関数を作成\n",
        "def answer_question(\n",
        "    df,\n",
        "    model=\"text-davinci-003\",\n",
        "    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n",
        "    max_len=1800,\n",
        "    size=\"ada\",\n",
        "    debug=True,\n",
        "    max_tokens=150,\n",
        "    stop_sequence=None\n",
        "):\n",
        "\n",
        "  #create_context関数を実行し、質問と類似度の高い文章を抽出\n",
        "  context = create_context(\n",
        "      question,\n",
        "      df,\n",
        "      max_len=max_len,\n",
        "      size=size,\n",
        "  )\n",
        "  # デバッグがTrueの場合返却された文章を表示\n",
        "  if debug:\n",
        "      print(\"Context:\\n\" + context)\n",
        "      print(\"\\n\\n\")\n",
        "\n",
        "  try:\n",
        "      #Questionとcontextを用いてAIモデルにリクエストを実行\n",
        "      response = openai.Completion.create(\n",
        "          prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n",
        "          temperature=0,\n",
        "          max_tokens=max_tokens,\n",
        "          top_p=1,\n",
        "          frequency_penalty=0,\n",
        "          presence_penalty=0,\n",
        "          stop=stop_sequence,\n",
        "          model=model,\n",
        "      )\n",
        "      #Json形式のレスポンスから回答を取得\n",
        "      return response[\"choices\"][0][\"text\"].strip()\n",
        "  except Exception as e:\n",
        "      print(e)\n",
        "      return \"\""
      ],
      "metadata": {
        "id": "77_bbLw7EnBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(df, question=\"Flutterで画面遷移を実現するにはどうすべきですか？手順を教えてください\")"
      ],
      "metadata": {
        "id": "hK7fop7-EoDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"Please tell me the starts of universe\"}\n",
        "  ],\n",
        "  max_tokens=150\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "id": "5-3EnHY5KKPm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}